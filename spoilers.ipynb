{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automated Spoiler Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Kristine Guo and Caroline Ho\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2018 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "0. [Overview](#Overview)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Baseline](#Baseline)\n",
    "  0. [Features](#Features)\n",
    "  0. [Experiment](#Experiment)\n",
    "0. [Dependency Parsing](#Dependency-Parsing)\n",
    "0. [Latent Dirichlet Allocation](#Latent-Dirichlet-Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "Manually tagging and avoiding spoilers online has become an increasingly difficult task due to the Internet's widespread social influence. We aim to tackle the problem of automatic spoiler detection, specifically applied to the TV Tropes dataset (Boyd-Graber et al., 2013). For our baseline, we train a linear kernel SVM that utilizes standard Bag of Words features (unigrams, stemming, bigrams). We improve upon this model by utilizing 1) dependency parsing, and 2) Latent Dirichlet Allocation (LDA). Our experiments revealed that dependency parsing and stemming produces the highest F1 score and that LDA negatively impacts performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set-up\n",
    "\n",
    "* Download [the TV Tropes dataset](http://cs.colorado.edu/~jbg/downloads/spoilers.tar.gz), unpack it, and place it in the current directory (or wherever you point `data_home` to below).\n",
    "\n",
    "* Download train_parsed_full.txt and place it in the current directory.\n",
    "\n",
    "* Download [PorterStemmer](https://tartarus.org/martin/PorterStemmer/python.txt) and save as a .py file in the current directory.\n",
    "\n",
    "* Download [the Stanford parser](https://nlp.stanford.edu/software/stanford-parser-full-2018-02-27.zip), unpack it, and place it in the current directory.\n",
    "\n",
    "* Download [English models for the Stanford parser](https://nlp.stanford.edu/software/stanford-english-corenlp-2018-02-27-models.jar), unpack the file, and place it in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import PorterStemmer\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.svm import LinearSVC\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = 'tvtropes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev1 = pd.read_csv(\n",
    "    os.path.join(data_home, 'dev1.balanced.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev2 = pd.read_csv(\n",
    "    os.path.join(data_home, 'dev2.balanced.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\n",
    "    os.path.join(data_home, 'test.balanced.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\n",
    "    os.path.join(data_home, 'train.balanced.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer.PorterStemmer()\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def parse_sentence(sentence):\n",
    "    s = sentence.translate(translator).split()\n",
    "    for i in range(len(s)):\n",
    "        s[i] = s[i].strip(string.punctuation).lower()\n",
    "    s = list(filter(None, s))\n",
    "    return [word for word in s if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Features\n",
    "\n",
    "Description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unigrams_phi(s):\n",
    "    return Counter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_phi(s):\n",
    "    return Counter([ps.stem(word) for word in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_phi(s):\n",
    "    t = copy.deepcopy(s)\n",
    "    t.insert(0, '<S>')\n",
    "    t.append('</S>')\n",
    "    bigrams = [tuple([t[i], t[i + 1]]) for i in range(len(t) - 1)]\n",
    "    return Counter(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Experiment\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_X(X, phi, vectorizer=None, should_parse=True):\n",
    "    feat_dicts = []\n",
    "    if should_parse: feat_dicts = [phi(parse_sentence(sentence)) for sentence in X]\n",
    "    else:\n",
    "        if len(X) == len(train): feat_dicts = copy.deepcopy(train_parsed_lowered)\n",
    "        else: feat_dicts = phi(X)\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=False)\n",
    "        return (vectorizer.fit_transform(feat_dicts), vectorizer)\n",
    "    else:\n",
    "        return (vectorizer.transform(feat_dicts), vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data, phi, vectorizer=None, should_parse=True):\n",
    "    X = [data.loc[i, 'sentence'] for i in range(len(data.index))]\n",
    "    y = [data.loc[i, 'spoiler'] for i in range(len(data.index))]\n",
    "    feat_matrix, vectorizer = vectorize_X(X, phi, vectorizer=vectorizer, should_parse=should_parse)\n",
    "    return (normalize(feat_matrix), y, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svc(X, y):\n",
    "    mod = LinearSVC()\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(phi, train_func, train_data, test_data, should_parse=True):\n",
    "    X, y, vectorizer = build_dataset(train_data, phi, should_parse=should_parse)\n",
    "    print(len(X[0]))\n",
    "    mod = train_func(X, y)\n",
    "    X_test, y_test, vectorizer = build_dataset(test_data, phi, vectorizer=vectorizer, should_parse=should_parse)\n",
    "    predictions = mod.predict(X_test)\n",
    "    print('Accuracy: %0.03f' % accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions, digits=3))\n",
    "    return f1_score(y_test, predictions, average = 'macro', pos_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18968\n",
      "Accuracy: 0.629\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.595     0.681     0.635       700\n",
      "       True      0.670     0.582     0.623       777\n",
      "\n",
      "avg / total      0.634     0.629     0.629      1477\n",
      "\n",
      "13520\n",
      "Accuracy: 0.634\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.604     0.663     0.632       700\n",
      "       True      0.667     0.609     0.637       777\n",
      "\n",
      "avg / total      0.637     0.634     0.634      1477\n",
      "\n",
      "110710\n",
      "Accuracy: 0.588\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.566     0.567     0.566       700\n",
      "       True      0.609     0.607     0.608       777\n",
      "\n",
      "avg / total      0.588     0.588     0.588      1477\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5872906157624602"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment(unigrams_phi, fit_svc, train, test)\n",
    "experiment(stemmed_phi, fit_svc, train, test)\n",
    "experiment(bigrams_phi, fit_svc, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_jar = 'stanford-parser-full-2018-02-27/stanford-parser.jar'\n",
    "path_to_models_jar = 'stanford-english-corenlp-2018-02-27-models.jar'\n",
    "\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train_parsed_lowered for dependency parsing.\n",
    "train_parsed = []\n",
    "f = open(\"train_parsed_full.txt\", \"rb\")\n",
    "while 1:\n",
    "    try:\n",
    "        o = pickle.load(f)\n",
    "    except EOFError:\n",
    "        break\n",
    "    train_parsed.append(o)\n",
    "f.close()\n",
    "\n",
    "def process(k):\n",
    "    arr = k.split('-')\n",
    "    arr = [ps.stem(a.lower()) for a in arr]\n",
    "    return '-'.join(arr)\n",
    "\n",
    "train_parsed_lowered = []\n",
    "for a in train_parsed:\n",
    "    temp = {}\n",
    "    for k, v in a.items():\n",
    "        p = process(k)\n",
    "        if p in temp: temp[p] += v\n",
    "        else: temp[p] = v\n",
    "    train_parsed_lowered.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_mass_phi(sentences):\n",
    "    trunc_sentences = [sentence[:min(len(sentence), 200)] for sentence in sentences]\n",
    "    feat_dicts = []\n",
    "    for dep_graphs in dep_parser.raw_parse_sents(trunc_sentences):\n",
    "        deps = []\n",
    "        for parse in dep_graphs:\n",
    "            for t in parse.triples():\n",
    "                deps.append(process(t[2][0] + '-' + t[0][0]))\n",
    "        feat_dicts.append(Counter(deps))\n",
    "    return feat_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147231\n",
      "Accuracy: 0.631\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.613     0.599     0.606       700\n",
      "       True      0.646     0.660     0.653       777\n",
      "\n",
      "avg / total      0.631     0.631     0.631      1477\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6295081725766134"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment(dep_mass_phi, fit_svc, train, test, should_parse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_build_dataset(data, phi, vectorizer=None, lda=None, should_parse=True):\n",
    "    X = [data.loc[i, 'sentence'] for i in range(len(data.index))]\n",
    "    y = [data.loc[i, 'spoiler'] for i in range(len(data.index))]\n",
    "    feat_matrix, vectorizer = vectorize_X(X, phi, vectorizer, should_parse=should_parse)\n",
    "    if lda==None:\n",
    "        lda = LatentDirichletAllocation(n_components=500)\n",
    "        feat_matrix = lda.fit_transform(feat_matrix)\n",
    "    else:\n",
    "        feat_matrix = lda.transform(feat_matrix)\n",
    "    return (normalize(feat_matrix), y, vectorizer, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_experiment(phi, train_func, train_data, test_data, should_parse=True):\n",
    "    X, y, vectorizer, lda = lda_build_dataset(train_data, phi, should_parse=should_parse)\n",
    "    print(len(X[0]))\n",
    "    mod = train_func(X, y)\n",
    "    X_test, y_test, vectorizer, lda = lda_build_dataset(test_data, phi, vectorizer=vectorizer, lda=lda, should_parse=should_parse)\n",
    "    predictions = mod.predict(X_test)\n",
    "    print('Accuracy: %0.03f' % accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions, digits=3))\n",
    "    return f1_score(y_test, predictions, average = 'macro', pos_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carolineho/anaconda3/envs/nlu/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "Accuracy: 0.563\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.546     0.464     0.502       700\n",
      "       True      0.575     0.653     0.611       777\n",
      "\n",
      "avg / total      0.561     0.563     0.559      1477\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5565710375836959"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_experiment(dep_mass_phi, fit_svc, train, test, should_parse=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
