{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Kristine Guo and Caroline Ho\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2018 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "0. [Overview](#Overview)\n",
    "0. [Set-up](#Set-up)\n",
    "0. [Baseline](#Baseline)\n",
    "  0. [Features](#Features)\n",
    "  0. [Experiment](#Experiment)\n",
    "0. [Sentiment](#Sentiment)\n",
    "0. [Dependency Parsing](#Dependency-Parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set-up\n",
    "\n",
    "* Make sure your environment meets all the requirements for [the cs224u repository](https://github.com/cgpotts/cs224u/). For help getting set-up, see [setup.ipynb](setup.ipynb).\n",
    "\n",
    "* Make sure you've downloaded [the data distribution for this unit](http://web.stanford.edu/class/cs224u/data/vsmdata.zip), unpacked it, and placed it in the current directory (or wherever you point `data_home` to below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import PorterStemmer\n",
    "import scipy.stats\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.svm import LinearSVC\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_home = 'tvtropes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev1 = pd.read_csv(\n",
    "    os.path.join(data_home, 'dev1.balanced.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev2 = pd.read_csv(\n",
    "    os.path.join(data_home, 'dev2.balanced.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\n",
    "    os.path.join(data_home, 'test.balanced.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\n",
    "    os.path.join(data_home, 'train.balanced.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorkCom\n"
     ]
    }
   ],
   "source": [
    "print(test.loc[0, 'trope'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer.PorterStemmer()\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def parse_sentence(sentence):\n",
    "    s = sentence.translate(translator).split()\n",
    "    for i in range(len(s)):\n",
    "        s[i] = s[i].strip(string.punctuation).lower()\n",
    "    s = list(filter(None, s))\n",
    "    return [word for word in s if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Features\n",
    "\n",
    "Description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unigrams_phi(s):\n",
    "    return Counter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemmed_phi(s):\n",
    "    return Counter([ps.stem(word) for word in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigrams_phi(s):\n",
    "    t = copy.deepcopy(s)\n",
    "    t.insert(0, '<S>')\n",
    "    t.append('</S>')\n",
    "    bigrams = [tuple([t[i], t[i + 1]]) for i in range(len(t) - 1)]\n",
    "    return Counter(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Experiment\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_X(X, phi, vectorizer=None, should_parse=True):\n",
    "    feat_dicts = []\n",
    "    if should_parse: feat_dicts = [phi(parse_sentence(sentence)) for sentence in X]\n",
    "    else: feat_dicts = [phi(sentence) for sentence in X]\n",
    "    if vectorizer == None:\n",
    "        vectorizer = DictVectorizer(sparse=False)\n",
    "        return (vectorizer.fit_transform(feat_dicts), vectorizer)\n",
    "    else:\n",
    "        return (vectorizer.transform(feat_dicts), vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, phi, vectorizer=None, should_parse=True):\n",
    "    X = [data.loc[i, 'sentence'] for i in range(len(data.index))]\n",
    "    y = [data.loc[i, 'spoiler'] for i in range(len(data.index))]\n",
    "    feat_matrix, vectorizer = vectorize_X(X, phi, vectorizer=vectorizer, should_parse=should_parse)\n",
    "    return (normalize(feat_matrix), y, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_svc(X, y):\n",
    "    mod = LinearSVC()\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment(phi, train_func, train_data, test_data, should_parse=True):\n",
    "    X, y, vectorizer = build_dataset(train_data, phi, should_parse=should_parse)\n",
    "    print(len(X[0]))\n",
    "    mod = train_func(X, y)\n",
    "    X_test, y_test, vectorizer = build_dataset(test_data, phi, vectorizer=vectorizer, should_parse=should_parse)\n",
    "    predictions = mod.predict(X_test)\n",
    "    print('Accuracy: %0.03f' % accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions, digits=3))\n",
    "    return f1_score(y_test, predictions, average = 'macro', pos_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18968\n",
      "Accuracy: 0.614\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.578     0.633     0.604       496\n",
      "       True      0.652     0.598     0.624       570\n",
      "\n",
      "avg / total      0.618     0.614     0.615      1066\n",
      "\n",
      "13520\n",
      "Accuracy: 0.618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.584     0.623     0.603       496\n",
      "       True      0.652     0.614     0.632       570\n",
      "\n",
      "avg / total      0.620     0.618     0.619      1066\n",
      "\n",
      "110710\n",
      "Accuracy: 0.598\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.568     0.567     0.567       496\n",
      "       True      0.623     0.625     0.624       570\n",
      "\n",
      "avg / total      0.598     0.598     0.598      1066\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5955589791028989"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment(unigrams_phi, fit_svc, train, dev1)\n",
    "experiment(stemmed_phi, fit_svc, train, dev1)\n",
    "experiment(bigrams_phi, fit_svc, train, dev1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_phi(s):\n",
    "    t = copy.deepcopy(s)\n",
    "    is_neg = 0\n",
    "    for i in range(len(t)):\n",
    "        if is_neg > 4: is_neg = 0\n",
    "        if is_neg > 0:\n",
    "            is_neg += 1\n",
    "            t[i] = 'NOT_' + t[i]\n",
    "        if 'n\\'t' in t[i] or t[i] in ['not', 'no', 'never']: is_neg = 1\n",
    "    return Counter(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment(sentiment_phi, fit_svc, train, dev1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_jar = 'stanford-parser-full-2018-02-27/stanford-parser.jar'\n",
    "path_to_models_jar = 'stanford-english-corenlp-2018-02-27-models.jar'\n",
    "\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(('jumps', 'VBZ'), 'nsubj', ('fox', 'NN')), (('fox', 'NN'), 'det', ('The', 'DT')), (('fox', 'NN'), 'amod', ('quick', 'JJ')), (('fox', 'NN'), 'amod', ('brown', 'JJ')), (('jumps', 'VBZ'), 'nmod', ('dog', 'NN')), (('dog', 'NN'), 'case', ('over', 'IN')), (('dog', 'NN'), 'det', ('the', 'DT')), (('dog', 'NN'), 'amod', ('lazy', 'JJ'))]] 1\n"
     ]
    }
   ],
   "source": [
    "x = [list(parse.triples()) for parse in dep_parser.raw_parse(\"The quick brown fox jumps over the lazy dog.\")]\n",
    "print(x, len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'fox-jumps': 1, 'The-fox': 1, 'quick-fox': 1, 'brown-fox': 1, 'dog-jumps': 1, 'over-dog': 1, 'the-dog': 1, 'lazy-dog': 1})\n"
     ]
    }
   ],
   "source": [
    "def dep_phi(sentence):\n",
    "    deps = []\n",
    "    for parse in dep_parser.raw_parse(sentence):\n",
    "        for t in parse.triples():\n",
    "            deps.append(t[2][0] + '-' + t[0][0])\n",
    "    return Counter(deps)\n",
    "\n",
    "print(dep_phi(\"The quick brown fox jumps over the lazy dog.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "Parsing file: /var/folders/tk/sdl353716c17z4dyf_77pmd00000gp/T/tmp_u83c9q8\n",
      "Parsing [sent. 1 len. 167]: Well ... Peter is in love with Olivia who liked John at first but he died and she liked Peter back but she does n't remember him as of season 4 and now has a thing for Lincoln who is quite smitten back but is friends with Peter who seems to ship them as well for some reason but Olivia remembered Peter again and now Lincoln is broken hearted and in the alternate time line Peter thought that Fauxlivia was Olivia and spent seven episodes enamoured with her but she was dating Frank but Alternate Lincoln liked her too and seemed to be fond of our Olivia as well when Walternate mind raped her into thinking she was Fauxlivia but in the amber timeline Fauxlivia has broken up with Frank and Lincoln was her `` shoulder to cry on '' but then he died but the other Lincoln said he 'd stay and help and Seth Gabel ships them and ... I think that 's it .\n",
      "Exception in thread \"main\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat edu.stanford.nlp.process.PTB2TextLexer.<init>(PTB2TextLexer.java:827)\n",
      "\tat edu.stanford.nlp.process.PTBTokenizer.ptb2Text(PTBTokenizer.java:333)\n",
      "\tat edu.stanford.nlp.process.PTBTokenizer.ptbToken2Text(PTBTokenizer.java:349)\n",
      "\tat edu.stanford.nlp.trees.TreePrint.printTreeInternal(TreePrint.java:581)\n",
      "\tat edu.stanford.nlp.trees.TreePrint.printTree(TreePrint.java:267)\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.processResults(ParseFiles.java:287)\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:216)\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:75)\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1518)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6141201960551175"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment(dep_phi, fit_svc, train, dev1, should_parse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, phi, vectorizer=None, lda=None):\n",
    "    X = [data.loc[i, 'sentence'] for i in range(len(data.index))]\n",
    "    y = [data.loc[i, 'spoiler'] for i in range(len(data.index))]\n",
    "    feat_matrix, vectorizer = vectorize_X(X, phi, vectorizer)\n",
    "    if lda==None:\n",
    "        lda = LatentDirichletAllocation(n_components=50)\n",
    "        feat_matrix = lda.fit_transform(feat_matrix)\n",
    "    else:\n",
    "        feat_matrix = lda.transform(feat_matrix)\n",
    "    return (normalize(feat_matrix), y, vectorizer, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment(phi, train_func, train_data, test_data):\n",
    "    X, y, vectorizer, lda = build_dataset(train_data, phi)\n",
    "    print(len(X[0]))\n",
    "    mod = train_func(X, y)\n",
    "    X_test, y_test, vectorizer, lda = build_dataset(test_data, phi, vectorizer=vectorizer, lda=lda)\n",
    "    predictions = mod.predict(X_test)\n",
    "    print('Accuracy: %0.03f' % accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions, digits=3))\n",
    "    return f1_score(y_test, predictions, average = 'macro', pos_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/nlu/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Accuracy: 0.541\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.510     0.367     0.427       496\n",
      "       True      0.557     0.693     0.618       570\n",
      "\n",
      "avg / total      0.535     0.541     0.529      1066\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5221996229102638"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment(unigrams_phi, fit_svc, train, dev1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
